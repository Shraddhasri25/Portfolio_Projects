{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing a LLM: Mistral-7B-Instruct"
      ],
      "metadata": {
        "id": "Uifv3B1Fe0EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Inferencing a Large Language Model?**\n",
        "Inferencing a model, especially in the context of Mistral 7B Instruct, refers to the process of using the pre-trained language model to generate predictions or responses based on input data or prompts. Here's a breakdown of the inferencing process:\n",
        "\n",
        "1. **Input Data or Prompt**: You provide input data or prompts to the Mistral 7B Instruct model. This input could be a text snippet, a question, or any content that you want the model to generate a response for.\n",
        "\n",
        "2. **Tokenization**: The input data is tokenized, which means it is divided into smaller units called tokens. Each token represents a specific part of the input data and is used by the model for processing.\n",
        "\n",
        "3. **Model Processing**: The tokenized input data is fed into the Mistral 7B Instruct model. The model processes this input using its pre-trained knowledge and language understanding capabilities.\n",
        "\n",
        "4. **Generation**: Based on the input data and the context provided by the pre-trained model, Mistral 7B Instruct generates a response or prediction. This response could be in the form of text, answers to questions, completion of sentences, or any other relevant output.\n",
        "\n",
        "5. **Decoding**: The generated output is decoded from the tokenized format back into human-readable text or the desired output format.\n",
        "\n",
        "6. **Output**: Finally, the model's generated output or inference is presented to the user, providing valuable insights, answers, or content based on the input provided.\n",
        "\n",
        "In essence, inferencing a model like Mistral 7B Instruct involves leveraging its language understanding capabilities to generate meaningful responses or predictions based on input data or prompts."
      ],
      "metadata": {
        "id": "1XcjftQldnZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Sharded Model?**\n",
        "\n",
        "A \"sharded\" model refers to a model that has been partitioned or split into multiple smaller parts called shards. This partitioning is typically done to facilitate more efficient processing and utilization of resources, especially in distributed computing environments or when dealing with very large models.\n",
        "\n",
        "In the context of Natural Language Processing (NLP) models like Mistral-7B-Instruct, sharding can be implemented for various reasons:\n",
        "\n",
        "1. **Parallelism**: Sharding allows different parts of the model to be processed in parallel on different computing units or devices. This can lead to faster inference times and better utilization of available computational resources.\n",
        "\n",
        "2. **Memory Optimization**: Large models may not fit entirely into the memory of a single device. Sharding helps distribute the model's components across memory spaces, enabling the use of models that would otherwise exceed memory limits.\n",
        "\n",
        "3. **Scalability**: Sharding supports the scalability of models, allowing them to handle larger volumes of data or more complex tasks by distributing the workload across multiple shards.\n",
        "\n",
        "4. **Efficient Training**: During model training, sharding can be used to distribute the training workload across multiple GPUs or machines, reducing training time and improving overall efficiency.\n",
        "\n",
        "5. **Fault Tolerance**: Sharding can also improve fault tolerance by isolating failures to specific shards, allowing the rest of the model to continue functioning.\n",
        "\n",
        "In summary, sharded models are designed to optimize performance, scalability, and resource utilization, especially in scenarios where large-scale models need to be deployed or trained efficiently."
      ],
      "metadata": {
        "id": "PwadkFzafkth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing necessary dependencies:**\n",
        "\n",
        "\n",
        "Here is an explanation of each library mentioned:\n",
        "\n",
        "1. `peft`: This library is not a standard library used in the Hugging Face Transformers package or related to it. It's possible that there might be a typo or misunderstanding regarding this library. If you meant to refer to another library or functionality, please provide more details.\n",
        "\n",
        "2. `accelerate`: Accelerate is a library designed to simplify and optimize the training and inference of PyTorch models, especially on distributed systems like multiple GPUs or TPUs. It provides utilities for distributed training, mixed precision training, gradient accumulation, and other performance optimizations.\n",
        "\n",
        "3. `bitsandbytes`: The `bitsandbytes` library is not directly related to the Hugging Face Transformers package. It might refer to a specific implementation or utility related to quantization or low-bit model representations. However, without more context or specific information, it's challenging to provide a detailed explanation.\n",
        "\n",
        "4. `safetensors`: This library is not a standard library related to the Hugging Face Transformers package. It could refer to a custom or specific utility for ensuring safe tensor operations, but without more information, it's challenging to provide specific details.\n",
        "\n",
        "5. `sentencepiece`: SentencePiece is a library for tokenization and subword encoding developed by Google. It's commonly used in Natural Language Processing (NLP) tasks, including pre-processing text data for training language models like those provided by the Hugging Face Transformers package. SentencePiece allows for efficient and customizable tokenization by splitting text into subword units based on the provided vocabulary.\n"
      ],
      "metadata": {
        "id": "GDUBRGyxeFpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQjrXRALZ3Q",
        "outputId": "16cd5097-78c2-4bf2-d8f2-aeff52b30fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -q peft  accelerate bitsandbytes safetensors sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'filipealmeida/Mistral-7B-Instruct-v0.1-sharded'\n",
        "\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,  # Use quantization_config instead of load_in_4bit\n",
        "        torch_dtype=torch.bfloat16,  # Remove load_in_4bit here\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# Define stop token ids\n",
        "stop_token_ids = [0]\n",
        "\n",
        "\n",
        "text = \"[INST] What is the future of AI? [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=600, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "ee281bd5dcb24c98b1e303c66adbfae6",
            "63a4cb1c5b3642018d7295de4c0ececa",
            "523559be158141b5aa163195da2c1161",
            "c12cc6f738db423093786da0bc1e3af7",
            "37b1d08d02a345b78a8dcca12abb1b98",
            "d9f928a86bd04a8291470af1e9a86c47",
            "4e0bdbc076b749b68d4987162f17063a",
            "81875adf12d144ecb82bcd0881f29e35",
            "2e65ecdcbf8b4ab587b0a5d2d3e1f138",
            "accf38cea79a49b18d1a3862da66afc1",
            "140b34451cb44814aba822762953877c"
          ]
        },
        "id": "Vqfe1685NLbD",
        "outputId": "100d3daa-c3ec-4c1c-ab9d-f6055bfadbfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee281bd5dcb24c98b1e303c66adbfae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1465: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] What is the future of AI? [/INST] It is difficult to predict the exact future of AI as it is constantly evolving based on new research and development. However, it is widely believed that AI will continue to play an increasingly important role in many aspects of our lives, both personal and professional. Some potential future developments in AI include:\n",
            "\n",
            "1. Greater integration with human emotions and behavior: AI may become more adept at detecting and responding to human emotions, leading to more natural and intuitive interactions between humans and AI.\n",
            "2. Continued advancements in natural language processing: AI may become even better at understanding and responding to natural language, allowing for more seamless communication with humans.\n",
            "3. Increased use in decision-making and problem-solving: AI may become more advanced in its ability to analyze data and make decisions, potentially replacing human decision-makers in certain situations.\n",
            "4. Greater use in healthcare: AI may be used to help diagnose and treat diseases, as well as to predict and prevent potential health problems.\n",
            "5. Increased use in transportation: Autonomous vehicles and drones may become more common, revolutionizing how we move goods and people.\n",
            "\n",
            "It's important to note that AI will likely continue to raise important ethical and safety questions, and these will need to be addressed as AI continues to advance.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a breakdown of what each part of the code does:\n",
        "\n",
        "1. `import torch`: Imports the PyTorch library, which is used for deep learning tasks.\n",
        "\n",
        "2. `from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig`: Imports necessary classes from the Transformers library. `AutoModelForCausalLM` is used to load the Causal Language Model, `AutoTokenizer` is used to initialize the tokenizer, and `BitsAndBytesConfig` is used for configuring quantization.\n",
        "\n",
        "3. `model_name = 'filipealmeida/Mistral-7B-Instruct-v0.1-sharded'`: Specifies the name or path of the model to be loaded.\n",
        "\n",
        "4. `load_quantized_model(model_name: str)`: Defines a function to load a quantized model based on the provided model name or path. Inside the function:\n",
        "   - `BitsAndBytesConfig` is used to configure quantization parameters.\n",
        "   - `AutoModelForCausalLM.from_pretrained` loads the model using the specified model name and quantization configuration.\n",
        "\n",
        "5. `initialize_tokenizer(model_name: str)`: Defines a function to initialize the tokenizer based on the provided model name or path. Inside the function:\n",
        "   - `AutoTokenizer.from_pretrained` initializes the tokenizer using the specified model name.\n",
        "   - `tokenizer.bos_token_id = 1` sets the beginning of sentence token ID.\n",
        "\n",
        "6. `model = load_quantized_model(model_name)`: Calls the `load_quantized_model` function to load the quantized model.\n",
        "\n",
        "7. `tokenizer = initialize_tokenizer(model_name)`: Calls the `initialize_tokenizer` function to initialize the tokenizer.\n",
        "\n",
        "8. `text = \"[INST] What is the future of AI? [/INST]\"`: Specifies the input text for which the model will generate a response.\n",
        "\n",
        "9. Tokenization and Generation:\n",
        "   - `encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)` tokenizes the input text without adding special tokens and returns a PyTorch tensor.\n",
        "   - `model.generate(**model_input, max_new_tokens=600, do_sample=True)` generates new text based on the encoded input using the loaded model and specified generation parameters.\n",
        "   - `tokenizer.batch_decode(generated_ids)` decodes the generated tokens back into human-readable text.\n",
        "   - `print(decoded[0])` prints the generated text.\n",
        "\n"
      ],
      "metadata": {
        "id": "IGWtpdejerMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TIP:  Use T4 GPU or TPU in Google Collab.**"
      ],
      "metadata": {
        "id": "IuMHP2nefw9B"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee281bd5dcb24c98b1e303c66adbfae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63a4cb1c5b3642018d7295de4c0ececa",
              "IPY_MODEL_523559be158141b5aa163195da2c1161",
              "IPY_MODEL_c12cc6f738db423093786da0bc1e3af7"
            ],
            "layout": "IPY_MODEL_37b1d08d02a345b78a8dcca12abb1b98"
          }
        },
        "63a4cb1c5b3642018d7295de4c0ececa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f928a86bd04a8291470af1e9a86c47",
            "placeholder": "​",
            "style": "IPY_MODEL_4e0bdbc076b749b68d4987162f17063a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "523559be158141b5aa163195da2c1161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81875adf12d144ecb82bcd0881f29e35",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e65ecdcbf8b4ab587b0a5d2d3e1f138",
            "value": 8
          }
        },
        "c12cc6f738db423093786da0bc1e3af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_accf38cea79a49b18d1a3862da66afc1",
            "placeholder": "​",
            "style": "IPY_MODEL_140b34451cb44814aba822762953877c",
            "value": " 8/8 [01:08&lt;00:00,  7.64s/it]"
          }
        },
        "37b1d08d02a345b78a8dcca12abb1b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f928a86bd04a8291470af1e9a86c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0bdbc076b749b68d4987162f17063a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81875adf12d144ecb82bcd0881f29e35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e65ecdcbf8b4ab587b0a5d2d3e1f138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "accf38cea79a49b18d1a3862da66afc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140b34451cb44814aba822762953877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}